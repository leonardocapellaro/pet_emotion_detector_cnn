{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import dlib\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut out the animal's face and save the resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_path, input_dir, output_dir):\n",
    "    try:\n",
    "        detector = dlib.cnn_face_detection_model_v1('dlib_models/dogHeadDetector.dat')\n",
    "        filename, ext = os.path.splitext(os.path.basename(img_path))\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        dets = detector(img, upsample_num_times=1)\n",
    "        \n",
    "        for i, d in enumerate(dets):\n",
    "\n",
    "            x1, y1 = d.rect.left(), d.rect.top()\n",
    "            x2, y2 = d.rect.right(), d.rect.bottom()\n",
    "            \n",
    "            roi = img[y1:y2, x1:x2]\n",
    "            \n",
    "            output_subdir = os.path.join(output_dir, os.path.relpath(os.path.dirname(img_path), input_dir))\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "            output_filename = f\"{filename}_face{i+1}.jpg\"\n",
    "            output_path = os.path.join(output_subdir, output_filename)\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(roi, cv2.COLOR_RGB2BGR))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {img_path}: {str(e)}\")\n",
    "\n",
    "def process_images_in_directory(input_dir, output_dir):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        image_paths = []\n",
    "        for root, dirs, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    img_path = os.path.join(root, file)\n",
    "                    image_paths.append(img_path)\n",
    "\n",
    "        pool.starmap(process_image, [(img_path, input_dir, output_dir) for img_path in image_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw images processing\n",
    "input_directory = \"images\"\n",
    "output_directory = \"images_processed\"\n",
    "process_images_in_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 00:45:03.423776: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-26 00:45:03.425664: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-26 00:45:03.455381: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-26 00:45:03.455937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-26 00:45:04.146612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/leonardocapellaro/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'images_processed'\n",
    "train_dir = 'images_processed/train'\n",
    "test_dir = 'images_processed/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_old_test_train(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)  # Use rmtree to remove a directory and it's content\n",
    "            print(f\"Directory '{path}' successfully removed.\")\n",
    "    else:\n",
    "        print(f\"Directory '{path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório 'images_processed/train' não existe.\n",
      "Diretório 'images_processed/test' não existe.\n"
     ]
    }
   ],
   "source": [
    "clean_old_test_train(train_dir)\n",
    "clean_old_test_train(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(file):\n",
    "    src_file = os.path.join(src_label_dir, file)\n",
    "    dst_file = os.path.join(train_label_dir, file)\n",
    "    \n",
    "    if not os.path.isfile(src_file):\n",
    "        return\n",
    "    \n",
    "    img = load_img(src_file)\n",
    "    img = img.resize((250, 250))\n",
    "    img.save(dst_file)\n",
    "    \n",
    "    x = img_to_array(img)\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size=1,\n",
    "                              save_to_dir=train_label_dir, save_prefix='aug', save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = os.listdir(src_dir)  #Obter as labels a partir dos subdiretórios\n",
    "\n",
    "for label in labels:\n",
    "    src_label_dir = os.path.join(src_dir, label)\n",
    "    train_label_dir = os.path.join(train_dir, label)\n",
    "    test_label_dir = os.path.join(test_dir, label)\n",
    "    \n",
    "    os.makedirs(train_label_dir, exist_ok=True)\n",
    "    os.makedirs(test_label_dir, exist_ok=True)\n",
    "    \n",
    "    files = os.listdir(src_label_dir)\n",
    "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    with multiprocessing.Pool() as p:\n",
    "        p.map(generate_and_save_images, train_files)\n",
    "    \n",
    "    for file in test_files:\n",
    "        src_file = os.path.join(src_label_dir, file)\n",
    "        dst_file = os.path.join(test_label_dir, file)\n",
    "        copyfile(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8639 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(250, 250),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 949 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(250, 250),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet 121 - With LR decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model_densenet_121(train_generator, test_generator, num_epochs=30, initial_learning_rate=0.001, final_learning_rate=1e-5):\n",
    "    \n",
    "    #Callback EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                                   patience=15,\n",
    "                                   verbose=1,\n",
    "                                   restore_best_weights=True)\n",
    "    \n",
    "    densenet_base = tf.keras.applications.DenseNet121(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape=(250, 250, 3))\n",
    "    \n",
    "    #Freeze the layers of DenseNet121\n",
    "    densenet_base.trainable = False\n",
    "    \n",
    "    #Generation of the steps value\n",
    "    total_training_examples = int(train_generator.samples)\n",
    "    batch_size = train_generator.batch_size\n",
    "    steps_per_epoch = math.ceil(total_training_examples / batch_size)\n",
    "    validation_steps = test_generator.samples // test_generator.batch_size\n",
    "    \n",
    "\n",
    "    model_densenet = tf.keras.models.Sequential([\n",
    "        densenet_base,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    #Decay learning rate\n",
    "    decay_rate = (final_learning_rate / initial_learning_rate) ** (1 / num_epochs)\n",
    "    \n",
    "    #Learning rate scheduling function\n",
    "    def lr_schedule(epoch):\n",
    "        current_learning_rate = initial_learning_rate * (decay_rate ** epoch)\n",
    "        return current_learning_rate\n",
    "    \n",
    "    model_densenet.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    #lr_schedule function to schedule the learning rate\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    history_densenet = model_densenet.fit(train_generator,\n",
    "                                          steps_per_epoch=steps_per_epoch,\n",
    "                                          epochs=num_epochs,\n",
    "                                          validation_data=test_generator,\n",
    "                                          validation_steps=validation_steps,\n",
    "                                          callbacks=[early_stopping, lr_callback])\n",
    "    \n",
    "    return model_densenet, history_densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 00:47:16.637687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-26 00:47:16.638966: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "270/270 [==============================] - 369s 1s/step - loss: 4.0865 - accuracy: 0.5787 - val_loss: 0.8466 - val_accuracy: 0.6681 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "270/270 [==============================] - 351s 1s/step - loss: 0.7707 - accuracy: 0.6927 - val_loss: 0.8891 - val_accuracy: 0.6746 - lr: 9.5499e-04\n",
      "Epoch 3/100\n",
      "270/270 [==============================] - 351s 1s/step - loss: 0.6970 - accuracy: 0.7223 - val_loss: 0.8234 - val_accuracy: 0.6853 - lr: 9.1201e-04\n",
      "Epoch 4/100\n",
      "270/270 [==============================] - 352s 1s/step - loss: 0.6375 - accuracy: 0.7468 - val_loss: 0.8959 - val_accuracy: 0.6466 - lr: 8.7096e-04\n",
      "Epoch 5/100\n",
      "270/270 [==============================] - 352s 1s/step - loss: 0.5532 - accuracy: 0.7789 - val_loss: 0.8939 - val_accuracy: 0.6724 - lr: 8.3176e-04\n",
      "Epoch 6/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.5244 - accuracy: 0.7886 - val_loss: 0.9634 - val_accuracy: 0.6724 - lr: 7.9433e-04\n",
      "Epoch 7/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.4590 - accuracy: 0.8119 - val_loss: 0.9294 - val_accuracy: 0.6746 - lr: 7.5858e-04\n",
      "Epoch 8/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.4523 - accuracy: 0.8114 - val_loss: 0.9187 - val_accuracy: 0.6853 - lr: 7.2444e-04\n",
      "Epoch 9/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.4236 - accuracy: 0.8318 - val_loss: 0.9624 - val_accuracy: 0.6940 - lr: 6.9183e-04\n",
      "Epoch 10/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.4078 - accuracy: 0.8352 - val_loss: 1.0243 - val_accuracy: 0.6897 - lr: 6.6069e-04\n",
      "Epoch 11/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.3582 - accuracy: 0.8516 - val_loss: 1.1335 - val_accuracy: 0.6756 - lr: 6.3096e-04\n",
      "Epoch 12/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.3415 - accuracy: 0.8584 - val_loss: 1.1274 - val_accuracy: 0.6724 - lr: 6.0256e-04\n",
      "Epoch 13/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.3042 - accuracy: 0.8726 - val_loss: 1.0739 - val_accuracy: 0.6940 - lr: 5.7544e-04\n",
      "Epoch 14/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.2915 - accuracy: 0.8797 - val_loss: 1.1467 - val_accuracy: 0.6918 - lr: 5.4954e-04\n",
      "Epoch 15/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.2609 - accuracy: 0.8898 - val_loss: 1.2823 - val_accuracy: 0.6573 - lr: 5.2481e-04\n",
      "Epoch 16/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.2665 - accuracy: 0.8873 - val_loss: 1.3093 - val_accuracy: 0.6832 - lr: 5.0119e-04\n",
      "Epoch 17/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.2468 - accuracy: 0.8967 - val_loss: 1.1885 - val_accuracy: 0.6929 - lr: 4.7863e-04\n",
      "Epoch 18/100\n",
      "270/270 [==============================] - 350s 1s/step - loss: 0.2409 - accuracy: 0.9016 - val_loss: 1.4844 - val_accuracy: 0.6735 - lr: 4.5709e-04\n",
      "Epoch 19/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.2193 - accuracy: 0.9105 - val_loss: 1.4545 - val_accuracy: 0.6875 - lr: 4.3652e-04\n",
      "Epoch 20/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.2041 - accuracy: 0.9132 - val_loss: 1.5626 - val_accuracy: 0.6918 - lr: 4.1687e-04\n",
      "Epoch 21/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.1955 - accuracy: 0.9162 - val_loss: 1.6604 - val_accuracy: 0.6897 - lr: 3.9811e-04\n",
      "Epoch 22/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.1858 - accuracy: 0.9193 - val_loss: 1.7432 - val_accuracy: 0.6767 - lr: 3.8019e-04\n",
      "Epoch 23/100\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.1900 - accuracy: 0.9170 - val_loss: 1.7410 - val_accuracy: 0.6886 - lr: 3.6308e-04\n",
      "Epoch 24/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.9246Restoring model weights from the end of the best epoch: 9.\n",
      "270/270 [==============================] - 353s 1s/step - loss: 0.1758 - accuracy: 0.9246 - val_loss: 1.8512 - val_accuracy: 0.6843 - lr: 3.4674e-04\n",
      "Epoch 24: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_densenet_121, history_densenet_121 = create_and_train_model_densenet_121(train_generator, test_generator, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "949"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 32s - loss: 0.9631 - accuracy: 0.6934 - 32s/epoch - 1s/step\n",
      "\n",
      "Test accuracy: 0.6933614611625671\n"
     ]
    }
   ],
   "source": [
    "test_loss_densenet_121, test_acc_densenet_121 = model_densenet_121.evaluate(test_generator, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc_densenet_121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet 201 - With LR decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet201\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model_densenet_201(train_generator, test_generator, num_epochs=30, initial_learning_rate=0.001, final_learning_rate=1e-5):\n",
    "    \n",
    "    #Callback EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                                  patience=15,\n",
    "                                  verbose=1,\n",
    "                                  restore_best_weights=True)\n",
    "\n",
    "    \n",
    "    densenet_base = tf.keras.applications.DenseNet201(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape=(250, 250, 3))\n",
    "    \n",
    "    #Freeze the layers of DenseNet201\n",
    "    densenet_base.trainable = False\n",
    "\n",
    "    #Geração do valor de steps\n",
    "    total_training_examples = train_generator.samples\n",
    "    batch_size = train_generator.batch_size\n",
    "    steps_per_epoch = total_training_examples / batch_size\n",
    "    validation_steps = test_generator.samples // test_generator.batch_size\n",
    "\n",
    "    model_densenet = tf.keras.models.Sequential([\n",
    "        densenet_base,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    #Decay learning rate\n",
    "    decay_rate = (final_learning_rate / initial_learning_rate) ** (1 / num_epochs)\n",
    "    \n",
    "    #Learning rate scheduling function\n",
    "    def lr_schedule(epoch):\n",
    "        current_learning_rate = initial_learning_rate * (decay_rate ** epoch)\n",
    "        return current_learning_rate\n",
    "\n",
    "    model_densenet.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),  # Use learning_rate\n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    #lr_schedule function to schedule the learning rate\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    history_densenet = model_densenet.fit(train_generator,\n",
    "                                          steps_per_epoch=steps_per_epoch,\n",
    "                                          epochs=num_epochs,\n",
    "                                          validation_data=test_generator,\n",
    "                                          validation_steps=validation_steps,\n",
    "                                          callbacks=[early_stopping, lr_callback])\n",
    "    \n",
    "    return model_densenet, history_densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "269/269 [==============================] - 557s 2s/step - loss: 3.2795 - accuracy: 0.6181 - val_loss: 0.9156 - val_accuracy: 0.6121 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "269/269 [==============================] - 550s 2s/step - loss: 0.7102 - accuracy: 0.7154 - val_loss: 0.7776 - val_accuracy: 0.6875 - lr: 9.5499e-04\n",
      "Epoch 3/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.5700 - accuracy: 0.7746 - val_loss: 0.8019 - val_accuracy: 0.7080 - lr: 9.1201e-04\n",
      "Epoch 4/100\n",
      "269/269 [==============================] - 550s 2s/step - loss: 0.4831 - accuracy: 0.8084 - val_loss: 0.7792 - val_accuracy: 0.7091 - lr: 8.7096e-04\n",
      "Epoch 5/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.4431 - accuracy: 0.8209 - val_loss: 0.8405 - val_accuracy: 0.6918 - lr: 8.3176e-04\n",
      "Epoch 6/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.3945 - accuracy: 0.8419 - val_loss: 0.8537 - val_accuracy: 0.7155 - lr: 7.9433e-04\n",
      "Epoch 7/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.3432 - accuracy: 0.8624 - val_loss: 0.8750 - val_accuracy: 0.7144 - lr: 7.5858e-04\n",
      "Epoch 8/100\n",
      "269/269 [==============================] - 548s 2s/step - loss: 0.3065 - accuracy: 0.8760 - val_loss: 0.8972 - val_accuracy: 0.7101 - lr: 7.2444e-04\n",
      "Epoch 9/100\n",
      "269/269 [==============================] - 550s 2s/step - loss: 0.3013 - accuracy: 0.8758 - val_loss: 0.8909 - val_accuracy: 0.7101 - lr: 6.9183e-04\n",
      "Epoch 10/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.2881 - accuracy: 0.8836 - val_loss: 0.9860 - val_accuracy: 0.7177 - lr: 6.6069e-04\n",
      "Epoch 11/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.2570 - accuracy: 0.8896 - val_loss: 1.0345 - val_accuracy: 0.7166 - lr: 6.3096e-04\n",
      "Epoch 12/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.2496 - accuracy: 0.8957 - val_loss: 0.9479 - val_accuracy: 0.7274 - lr: 6.0256e-04\n",
      "Epoch 13/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.2347 - accuracy: 0.9028 - val_loss: 1.0414 - val_accuracy: 0.7220 - lr: 5.7544e-04\n",
      "Epoch 14/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.2118 - accuracy: 0.9117 - val_loss: 1.0275 - val_accuracy: 0.7306 - lr: 5.4954e-04\n",
      "Epoch 15/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.2153 - accuracy: 0.9077 - val_loss: 1.1608 - val_accuracy: 0.7295 - lr: 5.2481e-04\n",
      "Epoch 16/100\n",
      "269/269 [==============================] - 550s 2s/step - loss: 0.1814 - accuracy: 0.9196 - val_loss: 1.1200 - val_accuracy: 0.7134 - lr: 5.0119e-04\n",
      "Epoch 17/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.1872 - accuracy: 0.9162 - val_loss: 1.3343 - val_accuracy: 0.7263 - lr: 4.7863e-04\n",
      "Epoch 18/100\n",
      "269/269 [==============================] - 548s 2s/step - loss: 0.1754 - accuracy: 0.9208 - val_loss: 1.2606 - val_accuracy: 0.7241 - lr: 4.5709e-04\n",
      "Epoch 19/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.1573 - accuracy: 0.9274 - val_loss: 1.3345 - val_accuracy: 0.7252 - lr: 4.3652e-04\n",
      "Epoch 20/100\n",
      "269/269 [==============================] - 549s 2s/step - loss: 0.1425 - accuracy: 0.9339 - val_loss: 1.3679 - val_accuracy: 0.7177 - lr: 4.1687e-04\n",
      "Epoch 21/100\n",
      "269/269 [==============================] - 548s 2s/step - loss: 0.1473 - accuracy: 0.9338 - val_loss: 1.5487 - val_accuracy: 0.7004 - lr: 3.9811e-04\n",
      "Epoch 22/100\n",
      "269/269 [==============================] - 592s 2s/step - loss: 0.1505 - accuracy: 0.9297 - val_loss: 1.4340 - val_accuracy: 0.7134 - lr: 3.8019e-04\n",
      "Epoch 23/100\n",
      "269/269 [==============================] - 553s 2s/step - loss: 0.1310 - accuracy: 0.9397 - val_loss: 1.4053 - val_accuracy: 0.7295 - lr: 3.6308e-04\n",
      "Epoch 24/100\n",
      "269/269 [==============================] - 551s 2s/step - loss: 0.1325 - accuracy: 0.9369 - val_loss: 1.4878 - val_accuracy: 0.7166 - lr: 3.4674e-04\n",
      "Epoch 25/100\n",
      "269/269 [==============================] - 551s 2s/step - loss: 0.1237 - accuracy: 0.9384 - val_loss: 1.5103 - val_accuracy: 0.7295 - lr: 3.3113e-04\n",
      "Epoch 26/100\n",
      "269/269 [==============================] - 556s 2s/step - loss: 0.1185 - accuracy: 0.9435 - val_loss: 1.6520 - val_accuracy: 0.7209 - lr: 3.1623e-04\n",
      "Epoch 27/100\n",
      "269/269 [==============================] - 559s 2s/step - loss: 0.1170 - accuracy: 0.9457 - val_loss: 1.7151 - val_accuracy: 0.7155 - lr: 3.0200e-04\n",
      "Epoch 28/100\n",
      "269/269 [==============================] - 560s 2s/step - loss: 0.1213 - accuracy: 0.9413 - val_loss: 1.6129 - val_accuracy: 0.7241 - lr: 2.8840e-04\n",
      "Epoch 29/100\n",
      "270/269 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9455Restoring model weights from the end of the best epoch: 14.\n",
      "269/269 [==============================] - 562s 2s/step - loss: 0.1098 - accuracy: 0.9455 - val_loss: 1.7141 - val_accuracy: 0.7144 - lr: 2.7542e-04\n",
      "Epoch 29: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_densenet_201, history_densenet_201 = create_and_train_model_densenet_201(train_generator, test_generator, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 50s - loss: 1.0546 - accuracy: 0.7281 - 50s/epoch - 2s/step\n",
      "\n",
      "Test accuracy: 0.7281348705291748\n"
     ]
    }
   ],
   "source": [
    "test_loss_densenet_201, test_acc_densenet_201 = model_densenet_201.evaluate(test_generator, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc_densenet_201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    model.save('cnn_models/'+filename)\n",
    "\n",
    "def load_model(filename):\n",
    "    model = load_model('cnn_models/'+filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardocapellaro/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "save_model(model_densenet_121, 'model_densenet_121.h5')\n",
    "save_model(model_densenet_201, 'model_densenet_201.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_predict(img_path, target_size=(250, 250)):\n",
    "    try:\n",
    "        detector = dlib.cnn_face_detection_model_v1('dlib_models/dogHeadDetector.dat')\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        dets = detector(img, upsample_num_times=1)\n",
    "\n",
    "        # Se nenhuma detecção de face for encontrada, retorne uma imagem em branco\n",
    "        if len(dets) == 0:\n",
    "            print(\"No faces were found in the image\")\n",
    "            return np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # Desenhe retângulos nas detecções\n",
    "        for i, d in enumerate(dets):\n",
    "            x1, y1 = d.rect.left(), d.rect.top()\n",
    "            x2, y2 = d.rect.right(), d.rect.bottom()\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Converta a imagem resultante de volta para BGR para salvar ou exibir\n",
    "        img_result = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Redimensione a imagem resultante para o tamanho desejado\n",
    "        img_result = cv2.resize(img_result, target_size)\n",
    "\n",
    "        return img_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in image {img_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def image_class_predict(model, image_path):\n",
    "    classes = [\"angry\", \"happy\", \"sad\"]\n",
    "    # Carregue e pré-processe a imagem de entrada\n",
    "    img = process_image_predict(image_path)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0  # Normalização (se necessário)\n",
    "\n",
    "    # Faça a previsão\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    # Obtenha a classe prevista\n",
    "    class_index = np.argmax(predictions)\n",
    "\n",
    "    return class_index, classes[class_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result using Densenet 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to load the downloaded model in cnn_models folder\n",
    "#model_densenet_121 = load_model('model_densenet_121.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "A imagem é da classe: happy\n"
     ]
    }
   ],
   "source": [
    "image = 'example.jpeg'\n",
    "predicted_class_index, predicted_class = image_class_predict(model_densenet_121, image)\n",
    "\n",
    "print(f'The image is from the class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result using Densenet 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to load the downloaded model in cnn_models folder\n",
    "#model_densenet_201 = load_model('model_densenet_201.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "A imagem é da classe: happy\n"
     ]
    }
   ],
   "source": [
    "imagem = 'example.jpeg'\n",
    "predicted_class_index, predicted_class = image_class_predict(model_densenet_201, imagem)\n",
    "\n",
    "print(f'The image is from the class: {predicted_class}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
