{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recortar rosto do animal e salvar a imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_path, input_dir, output_dir):\n",
    "    try:\n",
    "        detector = dlib.cnn_face_detection_model_v1('dlib_models/dogHeadDetector.dat')\n",
    "        filename, ext = os.path.splitext(os.path.basename(img_path))\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        dets = detector(img, upsample_num_times=1)\n",
    "        \n",
    "        for i, d in enumerate(dets):\n",
    "\n",
    "            x1, y1 = d.rect.left(), d.rect.top()\n",
    "            x2, y2 = d.rect.right(), d.rect.bottom()\n",
    "            \n",
    "            roi = img[y1:y2, x1:x2]\n",
    "            \n",
    "            output_subdir = os.path.join(output_dir, os.path.relpath(os.path.dirname(img_path), input_dir))\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "            output_filename = f\"{filename}_face{i+1}.jpg\"\n",
    "            output_path = os.path.join(output_subdir, output_filename)\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(roi, cv2.COLOR_RGB2BGR))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na imagem {img_path}: {str(e)}\")\n",
    "\n",
    "def process_images_in_directory(input_dir, output_dir):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        image_paths = []\n",
    "        for root, dirs, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    img_path = os.path.join(root, file)\n",
    "                    image_paths.append(img_path)\n",
    "\n",
    "        pool.starmap(process_image, [(img_path, input_dir, output_dir) for img_path in image_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "input_directory = \"images\"\n",
    "output_directory = \"images_processed\"\n",
    "process_images_in_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train e Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'images_processed'\n",
    "train_dir = 'images_processed/train'\n",
    "test_dir = 'images_processed/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_old_test_train(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)  # Use rmtree para remover um diretório e seu conteúdo\n",
    "        print(f\"Diretório '{path}' removido com sucesso.\")\n",
    "    else:\n",
    "        print(f\"Diretório '{dpath}' não existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório 'images_processed/train' removido com sucesso.\n",
      "Diretório 'images_processed/test' removido com sucesso.\n"
     ]
    }
   ],
   "source": [
    "clean_old_test_train(train_dir)\n",
    "clean_old_test_train(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = os.listdir(src_dir)  # Isso irá obter todas as labels a partir dos subdiretórios\n",
    "\n",
    "for label in labels:\n",
    "    src_label_dir = os.path.join(src_dir, label)\n",
    "    train_label_dir = os.path.join(train_dir, label)\n",
    "    test_label_dir = os.path.join(test_dir, label)\n",
    "    \n",
    "    os.makedirs(train_label_dir, exist_ok=True)\n",
    "    os.makedirs(test_label_dir, exist_ok=True)\n",
    "    \n",
    "    files = os.listdir(src_label_dir)\n",
    "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for file in train_files:\n",
    "        src_file = os.path.join(src_label_dir, file)\n",
    "        dst_file = os.path.join(train_label_dir, file)\n",
    "        copyfile(src_file, dst_file)\n",
    "    \n",
    "    for file in test_files:\n",
    "        src_file = os.path.join(src_label_dir, file)\n",
    "        dst_file = os.path.join(test_label_dir, file)\n",
    "        copyfile(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 01:51:29.594593: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-24 01:51:29.595899: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-24 01:51:29.624373: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-24 01:51:29.624677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-24 01:51:30.243406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4851 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1215 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(150, 150),\n",
    "                                                  batch_size=20,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(train_generator, test_generator, num_epochs=30):\n",
    "    \n",
    "    #Callback EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=10,\n",
    "                                   verbose=1,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "    #Definição da rede\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_generator,\n",
    "                        steps_per_epoch=100,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=test_generator,\n",
    "                        validation_steps=50,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 01:51:34.791715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-24 01:51:34.792059: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 19s 182ms/step - loss: 1.1513 - accuracy: 0.3910 - val_loss: 1.0984 - val_accuracy: 0.4280\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 19s 185ms/step - loss: 1.0988 - accuracy: 0.4320 - val_loss: 1.1041 - val_accuracy: 0.4230\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 19s 192ms/step - loss: 1.1067 - accuracy: 0.3887 - val_loss: 1.1194 - val_accuracy: 0.3900\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 1.1060 - accuracy: 0.4199 - val_loss: 1.1084 - val_accuracy: 0.4280\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 19s 192ms/step - loss: 1.1036 - accuracy: 0.4093 - val_loss: 1.1167 - val_accuracy: 0.4180\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 20s 197ms/step - loss: 1.0939 - accuracy: 0.4145 - val_loss: 1.0918 - val_accuracy: 0.4340\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 19s 194ms/step - loss: 1.1152 - accuracy: 0.4063 - val_loss: 1.0914 - val_accuracy: 0.4220\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 20s 197ms/step - loss: 1.0961 - accuracy: 0.4335 - val_loss: 1.1097 - val_accuracy: 0.3830\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 20s 198ms/step - loss: 1.0921 - accuracy: 0.4090 - val_loss: 1.1054 - val_accuracy: 0.3850\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 20s 197ms/step - loss: 1.0964 - accuracy: 0.4185 - val_loss: 1.1042 - val_accuracy: 0.3850\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 21s 212ms/step - loss: 1.1015 - accuracy: 0.4080 - val_loss: 1.1067 - val_accuracy: 0.3760\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 22s 219ms/step - loss: 1.0847 - accuracy: 0.4234 - val_loss: 1.0945 - val_accuracy: 0.4250\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 21s 210ms/step - loss: 1.0940 - accuracy: 0.4095 - val_loss: 1.1042 - val_accuracy: 0.4140\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 1.1047 - accuracy: 0.4249 - val_loss: 1.1023 - val_accuracy: 0.4170\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 1.1052 - accuracy: 0.4130 - val_loss: 1.0898 - val_accuracy: 0.4220\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 1.0951 - accuracy: 0.4120 - val_loss: 1.1019 - val_accuracy: 0.3830\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 21s 210ms/step - loss: 1.1074 - accuracy: 0.4159 - val_loss: 1.0967 - val_accuracy: 0.4090\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 20s 203ms/step - loss: 1.0940 - accuracy: 0.4274 - val_loss: 1.1079 - val_accuracy: 0.4100\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 20s 203ms/step - loss: 1.0997 - accuracy: 0.4105 - val_loss: 1.0939 - val_accuracy: 0.4270\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 20s 204ms/step - loss: 1.0790 - accuracy: 0.4120 - val_loss: 1.0991 - val_accuracy: 0.3820\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 20s 203ms/step - loss: 1.1056 - accuracy: 0.4219 - val_loss: 1.1020 - val_accuracy: 0.4340\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 20s 202ms/step - loss: 1.0928 - accuracy: 0.4095 - val_loss: 1.0952 - val_accuracy: 0.4280\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 20s 201ms/step - loss: 1.0938 - accuracy: 0.4013 - val_loss: 1.1030 - val_accuracy: 0.4250\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 20s 200ms/step - loss: 1.1041 - accuracy: 0.4175 - val_loss: 1.0932 - val_accuracy: 0.4250\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.1070 - accuracy: 0.4068Restoring model weights from the end of the best epoch: 15.\n",
      "100/100 [==============================] - 20s 201ms/step - loss: 1.1070 - accuracy: 0.4068 - val_loss: 1.0977 - val_accuracy: 0.4280\n",
      "Epoch 25: early stopping\n"
     ]
    }
   ],
   "source": [
    "model, history = create_and_train_model(train_generator, test_generator, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 - 3s - loss: 1.0967 - accuracy: 0.4239 - 3s/epoch - 47ms/step\n",
      "\n",
      "Test accuracy: 0.4238682985305786\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model_densenet(train_generator, test_generator, num_epochs=30):\n",
    "    \n",
    "    #Callback EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=10,\n",
    "                                   verbose=1,\n",
    "                                   restore_best_weights=True)\n",
    "    \n",
    "    densenet_base = tf.keras.applications.DenseNet121(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape=(150, 150, 3))\n",
    "    \n",
    "    # Congelar as camadas do DenseNet121\n",
    "    densenet_base.trainable = False\n",
    "\n",
    "    model_densenet = tf.keras.models.Sequential([\n",
    "        densenet_base,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model_densenet.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    history_densenet = model_densenet.fit(train_generator,\n",
    "                                          steps_per_epoch=100,\n",
    "                                          epochs=30,\n",
    "                                          validation_data=test_generator,\n",
    "                                          validation_steps=50,\n",
    "                                          callbacks=[early_stopping])\n",
    "    \n",
    "    return model_densenet, history_densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 48s 442ms/step - loss: 3.0099 - accuracy: 0.4880 - val_loss: 1.0313 - val_accuracy: 0.5990\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 1.0559 - accuracy: 0.5530 - val_loss: 0.9632 - val_accuracy: 0.5980\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.9466 - accuracy: 0.5846 - val_loss: 0.9502 - val_accuracy: 0.6130\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.9233 - accuracy: 0.5985 - val_loss: 0.8887 - val_accuracy: 0.6390\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.9371 - accuracy: 0.6110 - val_loss: 0.8723 - val_accuracy: 0.6360\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.8994 - accuracy: 0.6113 - val_loss: 0.8845 - val_accuracy: 0.6290\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.8782 - accuracy: 0.6070 - val_loss: 1.1042 - val_accuracy: 0.5500\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.9147 - accuracy: 0.6105 - val_loss: 0.9276 - val_accuracy: 0.6180\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.8950 - accuracy: 0.6173 - val_loss: 0.8933 - val_accuracy: 0.6300\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.8816 - accuracy: 0.6334 - val_loss: 0.8515 - val_accuracy: 0.6570\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8660 - accuracy: 0.6385 - val_loss: 0.8568 - val_accuracy: 0.6590\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.8584 - accuracy: 0.6425 - val_loss: 0.8950 - val_accuracy: 0.6200\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8452 - accuracy: 0.6440 - val_loss: 0.8707 - val_accuracy: 0.6560\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.8859 - accuracy: 0.6315 - val_loss: 0.8679 - val_accuracy: 0.6540\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.8456 - accuracy: 0.6510 - val_loss: 0.8657 - val_accuracy: 0.6440\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.8617 - accuracy: 0.6285 - val_loss: 0.9529 - val_accuracy: 0.5980\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.8554 - accuracy: 0.6424 - val_loss: 0.9958 - val_accuracy: 0.6020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8311 - accuracy: 0.6630 - val_loss: 0.8813 - val_accuracy: 0.6330\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8501 - accuracy: 0.6365 - val_loss: 0.8356 - val_accuracy: 0.6560\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.8753 - accuracy: 0.6334 - val_loss: 0.8567 - val_accuracy: 0.6330\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.8506 - accuracy: 0.6475 - val_loss: 0.8350 - val_accuracy: 0.6530\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.8618 - accuracy: 0.6429 - val_loss: 0.8438 - val_accuracy: 0.6480\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8315 - accuracy: 0.6580 - val_loss: 0.8284 - val_accuracy: 0.6690\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8465 - accuracy: 0.6444 - val_loss: 0.8464 - val_accuracy: 0.6600\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.8240 - accuracy: 0.6565 - val_loss: 0.8635 - val_accuracy: 0.6600\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8487 - accuracy: 0.6440 - val_loss: 0.8728 - val_accuracy: 0.6450\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.8696 - accuracy: 0.6313 - val_loss: 0.8298 - val_accuracy: 0.6720\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.8406 - accuracy: 0.6485 - val_loss: 0.8530 - val_accuracy: 0.6520\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.8352 - accuracy: 0.6424 - val_loss: 0.8366 - val_accuracy: 0.6710\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.8455 - accuracy: 0.6420 - val_loss: 0.8084 - val_accuracy: 0.6730\n"
     ]
    }
   ],
   "source": [
    "model_densenet, history_densenet = create_and_train_model_densenet(train_generator, test_generator, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 - 16s - loss: 0.8173 - accuracy: 0.6733 - 16s/epoch - 259ms/step\n",
      "\n",
      "Test accuracy: 0.6732510328292847\n"
     ]
    }
   ],
   "source": [
    "test_loss_densenet, test_acc_densenet = model_densenet.evaluate(test_generator, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc_densenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EffiicientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model_efficientnet(train_generator, test_generator, num_epochs=30):\n",
    "    \n",
    "    # Callback EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=10,\n",
    "                                   verbose=1,\n",
    "                                   restore_best_weights=True)\n",
    "    \n",
    "    efficientnet_base = EfficientNetB0(weights='imagenet',\n",
    "                                       include_top=False,\n",
    "                                       input_shape=(150, 150, 3))\n",
    "    \n",
    "    # Congelar as camadas do EfficientNetB0 para evitar que sejam treinadas\n",
    "    efficientnet_base.trainable = False\n",
    "\n",
    "    model_efficientnet = tf.keras.models.Sequential([\n",
    "        efficientnet_base,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model_efficientnet.compile(loss='categorical_crossentropy',\n",
    "                               optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "                               metrics=['accuracy'])\n",
    "\n",
    "    history_efficientnet = model_efficientnet.fit(train_generator,\n",
    "                                                  steps_per_epoch=100,\n",
    "                                                  epochs=num_epochs,\n",
    "                                                  validation_data=test_generator,\n",
    "                                                  validation_steps=50,\n",
    "                                                  callbacks=[early_stopping])\n",
    "    \n",
    "    return model_efficientnet, history_efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 27s 231ms/step - loss: 4.8206 - accuracy: 0.3702 - val_loss: 3.3875 - val_accuracy: 0.4260\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 22s 220ms/step - loss: 3.3088 - accuracy: 0.3646 - val_loss: 3.7003 - val_accuracy: 0.4210\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 22s 221ms/step - loss: 1.8791 - accuracy: 0.3913 - val_loss: 1.7857 - val_accuracy: 0.1820\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.9088 - accuracy: 0.3822 - val_loss: 2.2748 - val_accuracy: 0.1810\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.7560 - accuracy: 0.3596 - val_loss: 1.3180 - val_accuracy: 0.4190\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.5989 - accuracy: 0.3757 - val_loss: 1.4944 - val_accuracy: 0.1650\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.4403 - accuracy: 0.3855 - val_loss: 1.6241 - val_accuracy: 0.4250\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.3606 - accuracy: 0.3665 - val_loss: 1.5629 - val_accuracy: 0.4250\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.3023 - accuracy: 0.3785 - val_loss: 1.3420 - val_accuracy: 0.3770\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.2646 - accuracy: 0.3887 - val_loss: 1.2313 - val_accuracy: 0.3790\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1662 - accuracy: 0.4075 - val_loss: 1.2366 - val_accuracy: 0.4230\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.2172 - accuracy: 0.3933 - val_loss: 1.2148 - val_accuracy: 0.3820\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.2368 - accuracy: 0.3945 - val_loss: 1.2077 - val_accuracy: 0.4180\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.1871 - accuracy: 0.4135 - val_loss: 1.1857 - val_accuracy: 0.4240\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1500 - accuracy: 0.4105 - val_loss: 1.2060 - val_accuracy: 0.3720\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1503 - accuracy: 0.4028 - val_loss: 1.1861 - val_accuracy: 0.4200\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1565 - accuracy: 0.4005 - val_loss: 1.2309 - val_accuracy: 0.4140\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1460 - accuracy: 0.3940 - val_loss: 1.1358 - val_accuracy: 0.4260\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1322 - accuracy: 0.3893 - val_loss: 1.1622 - val_accuracy: 0.3800\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1323 - accuracy: 0.4033 - val_loss: 1.1207 - val_accuracy: 0.3910\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1286 - accuracy: 0.4179 - val_loss: 1.1133 - val_accuracy: 0.4160\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.1065 - accuracy: 0.4155 - val_loss: 1.1018 - val_accuracy: 0.4180\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.0855 - accuracy: 0.4214 - val_loss: 1.1021 - val_accuracy: 0.4160\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.1093 - accuracy: 0.3955 - val_loss: 1.0973 - val_accuracy: 0.4190\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.0969 - accuracy: 0.4068 - val_loss: 1.1034 - val_accuracy: 0.3980\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1017 - accuracy: 0.4008 - val_loss: 1.1056 - val_accuracy: 0.4200\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 1.1090 - accuracy: 0.4035 - val_loss: 1.1141 - val_accuracy: 0.3840\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.1109 - accuracy: 0.4174 - val_loss: 1.1032 - val_accuracy: 0.3760\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1073 - accuracy: 0.3995 - val_loss: 1.1049 - val_accuracy: 0.4170\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 1.1061 - accuracy: 0.4010 - val_loss: 1.0992 - val_accuracy: 0.4190\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.0813 - accuracy: 0.4185 - val_loss: 1.0996 - val_accuracy: 0.3810\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.1167 - accuracy: 0.4164 - val_loss: 1.0931 - val_accuracy: 0.4230\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.0973 - accuracy: 0.4205 - val_loss: 1.0949 - val_accuracy: 0.4300\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 22s 225ms/step - loss: 1.1266 - accuracy: 0.3882 - val_loss: 1.1043 - val_accuracy: 0.3970\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 22s 225ms/step - loss: 1.1064 - accuracy: 0.4180 - val_loss: 1.1361 - val_accuracy: 0.4200\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.0996 - accuracy: 0.3983 - val_loss: 1.1050 - val_accuracy: 0.4260\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.0933 - accuracy: 0.4234 - val_loss: 1.0983 - val_accuracy: 0.4260\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 1.1044 - accuracy: 0.4080 - val_loss: 1.1107 - val_accuracy: 0.4190\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 1.1121 - accuracy: 0.4110 - val_loss: 1.1000 - val_accuracy: 0.4220\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.1031 - accuracy: 0.4093 - val_loss: 1.1164 - val_accuracy: 0.3650\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 1.0990 - accuracy: 0.4220 - val_loss: 1.1020 - val_accuracy: 0.4240\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.0970 - accuracy: 0.4085Restoring model weights from the end of the best epoch: 32.\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 1.0970 - accuracy: 0.4085 - val_loss: 1.1050 - val_accuracy: 0.4320\n",
      "Epoch 42: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_efcnet, history_efcnet = create_and_train_model_efficientnet(train_generator, test_generator, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 - 6s - loss: 1.0969 - accuracy: 0.4239 - 6s/epoch - 99ms/step\n",
      "\n",
      "Test accuracy: 0.4238682985305786\n"
     ]
    }
   ],
   "source": [
    "test_loss_efcnet, test_acc_efcnet = model_efcnet.evaluate(test_generator, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc_efcnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    model.save('cnn_models/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardocapellaro/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "save_model(model, 'model.h5')\n",
    "save_model(model_densenet, 'model_densenet.h5')\n",
    "save_model(model_efcnet, 'model_efcnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predizer imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_predict(img_path, target_size=(150, 150)):\n",
    "    try:\n",
    "        detector = dlib.cnn_face_detection_model_v1('dlib_models/dogHeadDetector.dat')\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        dets = detector(img, upsample_num_times=1)\n",
    "\n",
    "        # Se nenhuma detecção de face for encontrada, retorne uma imagem em branco\n",
    "        if len(dets) == 0:\n",
    "            print(\"Não foram encontrados rostos na imagem\")\n",
    "            return np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # Desenhe retângulos nas detecções\n",
    "        for i, d in enumerate(dets):\n",
    "            x1, y1 = d.rect.left(), d.rect.top()\n",
    "            x2, y2 = d.rect.right(), d.rect.bottom()\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Converta a imagem resultante de volta para BGR para salvar ou exibir\n",
    "        img_result = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Redimensione a imagem resultante para o tamanho desejado\n",
    "        img_result = cv2.resize(img_result, target_size)\n",
    "\n",
    "        return img_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na imagem {img_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def image_class_predict(model, image_path):\n",
    "    classes = [\"angry\", \"happy\", \"sad\"]\n",
    "    # Carregue e pré-processe a imagem de entrada\n",
    "    img = process_image_predict(image_path)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0  # Normalização (se necessário)\n",
    "\n",
    "    # Faça a previsão\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    # Obtenha a classe prevista\n",
    "    class_index = np.argmax(predictions)\n",
    "\n",
    "    return class_index, classes[class_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado CNN Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "A imagem é da classe: happy\n"
     ]
    }
   ],
   "source": [
    "imagem = 'example.jpeg'\n",
    "classe_prevista_index, classe_prevista = image_class_predict(model, imagem)\n",
    "\n",
    "print(f'A imagem é da classe: {classe_prevista}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado Densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "A imagem é da classe: happy\n"
     ]
    }
   ],
   "source": [
    "imagem = 'example.jpeg'\n",
    "classe_prevista_index, classe_prevista = image_class_predict(model_densenet, imagem)\n",
    "\n",
    "print(f'A imagem é da classe: {classe_prevista}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado EFC Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 824ms/step\n",
      "A imagem é da classe: happy\n"
     ]
    }
   ],
   "source": [
    "imagem = 'example.jpeg'\n",
    "classe_prevista_index, classe_prevista = image_class_predict(model_efcnet, imagem)\n",
    "\n",
    "print(f'A imagem é da classe: {classe_prevista}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
